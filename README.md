Was curious if there is a relation between the entropy of the outputs of an LLM and accuracy on benchmarks. 

Turns out there's not(at least tested on llama3-8B-Instruct on GSM8K

results:

correct_entropy 11.370859312879782
incorrect_entropy 10.980136689315362
